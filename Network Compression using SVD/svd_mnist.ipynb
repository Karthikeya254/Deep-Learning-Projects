{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Compression using SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#Loading MNIST dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = mnist.train.images #Training Images\n",
    "train_y = mnist.train.labels # Training Labels\n",
    "test_x = mnist.test.images #Test Images\n",
    "test_y = mnist.test.labels #Test Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders\n",
    "Creating place holders for input data and output labels. I used mini-batches for gradient descent, as the mini-batch size is a variable, number of input samples is specified as None in the place holders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "Weights and bias parameters for 5 hidden layers with 1024 units each and one output layer which is a softmax layer with 10 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.get_variable(\"W1\", shape=[784, 1024], initializer=tf.keras.initializers.he_normal(seed=1))\n",
    "b1 = tf.get_variable(\"b1\", shape=[1024], initializer=tf.initializers.zeros)\n",
    "W2 = tf.get_variable(\"W2\", shape=[1024, 1024], initializer=tf.keras.initializers.he_normal(seed=1))\n",
    "b2 = tf.get_variable(\"b2\", shape=[1024], initializer=tf.initializers.zeros)\n",
    "W3 = tf.get_variable(\"W3\", shape=[1024, 1024], initializer=tf.keras.initializers.he_normal(seed=1))\n",
    "b3 = tf.get_variable(\"b3\", shape=[1024], initializer=tf.initializers.zeros)\n",
    "W4 = tf.get_variable(\"W4\", shape=[1024, 1024], initializer=tf.keras.initializers.he_normal(seed=1))\n",
    "b4 = tf.get_variable(\"b4\", shape=[1024], initializer=tf.initializers.zeros)\n",
    "W5 = tf.get_variable(\"W5\", shape=[1024, 1024], initializer=tf.keras.initializers.he_normal(seed=1))\n",
    "b5 = tf.get_variable(\"b5\", shape=[1024], initializer=tf.initializers.zeros)\n",
    "W6 = tf.get_variable(\"W6\", shape=[1024, 10], initializer=tf.keras.initializers.he_normal(seed=1))\n",
    "b6 = tf.get_variable(\"b6\", shape=[10], initializer=tf.initializers.zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "- Neural network model with 5 hidden layers with ReLU activation (Z1/A1, Z2/A2, Z3/A3, Z4/A4, Z5/A5) and a softmax layer.\n",
    "- Softmax crossentropy is used as cost function.\n",
    "- Training Model using Adagrad with learning rate, $\\alpha = 0.05$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1 = tf.add(tf.matmul(X, W1), b1) #num_samples x 1024\n",
    "A1 = tf.nn.relu(Z1)\n",
    "Z2 = tf.add(tf.matmul(A1, W2), b2) #num_samples x 1024\n",
    "A2 = tf.nn.relu(Z2)\n",
    "Z3 = tf.add(tf.matmul(A2, W3), b3) #num_samples x 1024\n",
    "A3 = tf.nn.relu(Z3)\n",
    "Z4 = tf.add(tf.matmul(A3, W4), b4) #num_samples x 1024\n",
    "A4 = tf.nn.relu(Z4)\n",
    "Z5 = tf.add(tf.matmul(A4, W5), b5) #num_samples x 1024\n",
    "A5 = tf.nn.relu(Z5)\n",
    "Z6 = tf.add(tf.matmul(A5, W6), b6) #num_samples x 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_crossent = tf.nn.softmax_cross_entropy_with_logits_v2(logits = Z6, labels = Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(softmax_crossent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_step = tf.train.GradientDescentOptimizer(0.05).minimize(cost)\n",
    "train_step = tf.train.AdagradOptimizer(0.05).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting configurtion for tensorflow memory usage\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.33\n",
    "\n",
    "#Creating a tendorflow session\n",
    "sess=tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing Model\n",
    "- Executing the neural network model that we built above. Using a mini-batch size of $100$ and running it for $200$ epochs.  \n",
    "- For every epoch the training dataset is reshuffled, so that the examples in each mini-batches will be random from epoch to epoch.  \n",
    "- Cost for each mini-batch in an epoch is divided by the number of batches, so that the total cost at the end of the epoch will be an average of all the mini-batch costs.\n",
    "- Training accuracy = $100\\%$\n",
    "- Test accuracy = $98.37\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 9: 0.000457\n",
      "Cost after epoch 19: 0.000064\n",
      "Cost after epoch 29: 0.000035\n",
      "Cost after epoch 39: 0.000024\n",
      "Cost after epoch 49: 0.000018\n",
      "Cost after epoch 59: 0.000015\n",
      "Cost after epoch 69: 0.000012\n",
      "Cost after epoch 79: 0.000011\n",
      "Cost after epoch 89: 0.000009\n",
      "Cost after epoch 99: 0.000008\n",
      "Cost after epoch 109: 0.000007\n",
      "Cost after epoch 119: 0.000007\n",
      "Cost after epoch 129: 0.000006\n",
      "Cost after epoch 139: 0.000006\n",
      "Cost after epoch 149: 0.000005\n",
      "Cost after epoch 159: 0.000005\n",
      "Cost after epoch 169: 0.000004\n",
      "Cost after epoch 179: 0.000004\n",
      "Cost after epoch 189: 0.000004\n",
      "Cost after epoch 199: 0.000004\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.9837\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "np.random.seed(1) #setting seed to reproduce same results\n",
    "batch_size = 100\n",
    "num_epochs = 200\n",
    "num_batches = math.ceil(train_x.shape[0]/batch_size)\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_cost = 0\n",
    "    perm = np.random.permutation(train_x.shape[0])\n",
    "    train_x = train_x[perm, :]\n",
    "    train_y = train_y[perm, :]\n",
    "    for i in range(num_batches):\n",
    "        start = i*batch_size\n",
    "        if i == num_batches-1:\n",
    "            batch_ip = train_x[start:train_x.shape[0], :]\n",
    "            batch_op = train_y[start:train_y.shape[0], :]\n",
    "        else:\n",
    "            batch_ip = train_x[start:start+batch_size, :]\n",
    "            batch_op = train_y[start:start+batch_size, :]\n",
    "        _, batch_cost = sess.run([train_step, cost], feed_dict={X:batch_ip, Y:batch_op})\n",
    "        epoch_cost += batch_cost/num_batches\n",
    "    if (epoch+1)%10 == 0:\n",
    "        print(\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "correct_preds = tf.equal(tf.argmax(Z6,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_preds, tf.float32))\n",
    "print(\"Train Accuracy:\", accuracy.eval(session = sess, feed_dict={X: mnist.train.images, Y: mnist.train.labels}))\n",
    "print(\"Test Accuracy:\", accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving weights into temporary variables\n",
    "W1_temp = W1\n",
    "W2_temp = W2\n",
    "W3_temp = W3\n",
    "W4_temp = W4\n",
    "W5_temp = W5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD\n",
    "Applying svd on each weight matrix for first 5 hidden layers and leacing the weights for softmax layer as it is. The (s,u,v) matrices obtained from svd are converted to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s11, u11, v11 = tf.svd(W1)\n",
    "s21, u21, v21 = tf.svd(W2)\n",
    "s31, u31, v31 = tf.svd(W3)\n",
    "s41, u41, v41 = tf.svd(W4)\n",
    "s51, u51, v51 = tf.svd(W5)\n",
    "\n",
    "#Converting to numpy arrays\n",
    "s1 = s11.eval(session=sess)\n",
    "u1 = u11.eval(session=sess)\n",
    "v1 = v11.eval(session=sess)\n",
    "s2 = s21.eval(session=sess)\n",
    "u2 = u21.eval(session=sess)\n",
    "v2 = v21.eval(session=sess)\n",
    "s3 = s31.eval(session=sess)\n",
    "u3 = u31.eval(session=sess)\n",
    "v3 = v31.eval(session=sess)\n",
    "s4 = s41.eval(session=sess)\n",
    "u4 = u41.eval(session=sess)\n",
    "v4 = v41.eval(session=sess)\n",
    "s5 = s51.eval(session=sess)\n",
    "u5 = u51.eval(session=sess)\n",
    "v5 = v51.eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 784)  --  (784,)  --  (784, 1024)\n",
      "(1024, 1024)  --  (1024,)  --  (1024, 1024)\n",
      "(1024, 1024)  --  (1024,)  --  (1024, 1024)\n",
      "(1024, 1024)  --  (1024,)  --  (1024, 1024)\n",
      "(1024, 1024)  --  (1024,)  --  (1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(u1.shape, \" -- \", s1.shape, \" -- \", v1.T.shape)\n",
    "print(u2.shape, \" -- \", s2.shape, \" -- \", v2.T.shape)\n",
    "print(u3.shape, \" -- \", s3.shape, \" -- \", v3.T.shape)\n",
    "print(u4.shape, \" -- \", s4.shape, \" -- \", v4.T.shape)\n",
    "print(u5.shape, \" -- \", s5.shape, \" -- \", v5.T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Compressed Weights\n",
    "- Computing the low rank weight matrices for each D value.\n",
    "- Replace the new weights in the graph and compute train and test accuracies\n",
    "\n",
    "|D|Train Acc|Test Acc|\n",
    "|:---:|---:|---:|\n",
    "|$10$|$68.15\\%$|$67.79\\%$|\n",
    "|$20$|$80.78\\%$|$80.14\\%$|\n",
    "|$50$|$89.03\\%$|$88.63\\%$|\n",
    "|$100$|$93.00\\%$|$92.65\\%$|\n",
    "|$200$|$96.70\\%$|$95.92\\%$|\n",
    "|$Full$|$100.00\\%$|$98.37\\%$|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_arr = [10,20,50,100,200,'FULL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For D =  10\n",
      "Train Accuracy: 0.6815091\n",
      "Test Accuracy: 0.6779\n",
      "Test exec time: 0.0218\n",
      "=========================================\n",
      "For D =  20\n",
      "Train Accuracy: 0.80785453\n",
      "Test Accuracy: 0.8014\n",
      "Test exec time: 0.014\n",
      "=========================================\n",
      "For D =  50\n",
      "Train Accuracy: 0.8903818\n",
      "Test Accuracy: 0.8863\n",
      "Test exec time: 0.0137\n",
      "=========================================\n",
      "For D =  100\n",
      "Train Accuracy: 0.9300909\n",
      "Test Accuracy: 0.9265\n",
      "Test exec time: 0.0137\n",
      "=========================================\n",
      "For D =  200\n",
      "Train Accuracy: 0.96703637\n",
      "Test Accuracy: 0.9592\n",
      "Test exec time: 0.0139\n",
      "=========================================\n",
      "For D =  FULL\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.9837\n",
      "Test exec time: 0.0142\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "for D in D_arr:\n",
    "    if D == 'FULL':\n",
    "        W1_hat = tf.matmul(tf.matmul(u1,tf.diag(s1)),tf.transpose(v1))\n",
    "        W2_hat = tf.matmul(tf.matmul(u2,tf.diag(s2)),tf.transpose(v2))\n",
    "        W3_hat = tf.matmul(tf.matmul(u3,tf.diag(s3)),tf.transpose(v3))\n",
    "        W4_hat = tf.matmul(tf.matmul(u4,tf.diag(s4)),tf.transpose(v4))\n",
    "        W5_hat = tf.matmul(tf.matmul(u5,tf.diag(s5)),tf.transpose(v5))\n",
    "    else:\n",
    "        W1_hat = tf.matmul(tf.matmul(u1[:,0:D],tf.diag(s1[0:D])),tf.transpose(v1[:,0:D]))\n",
    "        W2_hat = tf.matmul(tf.matmul(u2[:,0:D],tf.diag(s2[0:D])),tf.transpose(v2[:,0:D]))\n",
    "        W3_hat = tf.matmul(tf.matmul(u3[:,0:D],tf.diag(s3[0:D])),tf.transpose(v3[:,0:D]))\n",
    "        W4_hat = tf.matmul(tf.matmul(u4[:,0:D],tf.diag(s4[0:D])),tf.transpose(v4[:,0:D]))\n",
    "        W5_hat = tf.matmul(tf.matmul(u5[:,0:D],tf.diag(s5[0:D])),tf.transpose(v5[:,0:D]))\n",
    "\n",
    "    sess.run(tf.assign(sess.graph.get_tensor_by_name(\"W1:0\"), W1_hat))\n",
    "    sess.run(tf.assign(sess.graph.get_tensor_by_name('W2:0'), W2_hat))\n",
    "    sess.run(tf.assign(sess.graph.get_tensor_by_name('W3:0'), W3_hat))\n",
    "    sess.run(tf.assign(sess.graph.get_tensor_by_name('W4:0'), W4_hat))\n",
    "    sess.run(tf.assign(sess.graph.get_tensor_by_name('W5:0'), W5_hat))\n",
    "    print(\"For D = \", D)\n",
    "    print(\"Train Accuracy:\", accuracy.eval(session = sess, feed_dict={X: mnist.train.images, Y: mnist.train.labels}))\n",
    "    start = time.time()\n",
    "    print(\"Test Accuracy:\", accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))\n",
    "    stop = time.time()\n",
    "    print(\"Test exec time:\", round(stop-start,4))\n",
    "    print(\"=========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updated Network Parameters\n",
    "I choose the approach mentioned in 6.a to bilud the new network. The parameters for first 5 layers now are U, V which are determined by the earlier svd calculations and fixing D=20, and bias is initialized with the corresponding bias values in earlier network. The parameters for softmax layer (W6, b6) are initialized with corresponding values in earlier network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 20\n",
    "U1_D = tf.get_variable(\"U1_D\", initializer=u1[:,0:D])\n",
    "V1_D = tf.get_variable(\"V1_D\", initializer=np.matmul(np.diag(s1[0:D]), np.transpose(v1[:,0:D])))\n",
    "b1_D = tf.get_variable(\"b1_D\", initializer=b1)\n",
    "U2_D = tf.get_variable(\"U2_D\", initializer=u2[:,0:D])\n",
    "V2_D = tf.get_variable(\"V2_D\", initializer=np.matmul(np.diag(s2[0:D]), np.transpose(v2[:,0:D])))\n",
    "b2_D = tf.get_variable(\"b2_D\", initializer=b2)\n",
    "U3_D = tf.get_variable(\"U3_D\", initializer=u3[:,0:D])\n",
    "V3_D = tf.get_variable(\"V3_D\", initializer=np.matmul(np.diag(s3[0:D]), np.transpose(v3[:,0:D])))\n",
    "b3_D = tf.get_variable(\"b3_D\", initializer=b3)\n",
    "U4_D = tf.get_variable(\"U4_D\", initializer=u4[:,0:D])\n",
    "V4_D = tf.get_variable(\"V4_D\", initializer=np.matmul(np.diag(s4[0:D]), np.transpose(v4[:,0:D])))\n",
    "b4_D = tf.get_variable(\"b4_D\", initializer=b4)\n",
    "U5_D = tf.get_variable(\"U5_D\", initializer=u5[:,0:D])\n",
    "V5_D = tf.get_variable(\"V5_D\", initializer=np.matmul(np.diag(s5[0:D]), np.transpose(v5[:,0:D])))\n",
    "b5_D = tf.get_variable(\"b5_D\", initializer=b5)\n",
    "W6_D = tf.get_variable(\"W6_D\", initializer=W6)\n",
    "b6_D = tf.get_variable(\"b6_D\", initializer=b6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "- Neural network model with 10 hidden layers with ReLU activation after every second later output (U1,V1/A1, U2,V2/A2, U3,V3/A3, U4,V4/A4, U5,V5/A5) and a softmax layer.\n",
    "- Softmax crossentropy is used as cost function.\n",
    "- Training Model using Adagrad with learning rate, $\\alpha = 0.05$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "zu1 = tf.matmul(X, U1_D)\n",
    "zv1 = tf.add(tf.matmul(zu1, V1_D), b1_D)\n",
    "za1 = tf.nn.relu(zv1)\n",
    "zu2 = tf.matmul(za1, U2_D)\n",
    "zv2 = tf.add(tf.matmul(zu2, V2_D), b2_D)\n",
    "za2 = tf.nn.relu(zv2)\n",
    "zu3 = tf.matmul(za2, U3_D)\n",
    "zv3 = tf.add(tf.matmul(zu3, V3_D), b3_D)\n",
    "za3 = tf.nn.relu(zv3)\n",
    "zu4 = tf.matmul(za3, U4_D)\n",
    "zv4 = tf.add(tf.matmul(zu4, V4_D), b4_D)\n",
    "za4 = tf.nn.relu(zv4)\n",
    "zu5 = tf.matmul(za4, U5_D)\n",
    "zv5 = tf.add(tf.matmul(zu5, V5_D), b5_D)\n",
    "za5 = tf.nn.relu(zv5)\n",
    "zw6 = tf.add(tf.matmul(za5, W6_D), b6_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_crossent1 = tf.nn.softmax_cross_entropy_with_logits_v2(logits = zw6, labels = Y)\n",
    "cost1 = tf.reduce_mean(softmax_crossent1)\n",
    "train_step1 = tf.train.AdagradOptimizer(0.05).minimize(cost1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a tensorflow session\n",
    "sess=tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing Model\n",
    "- Executing the neural network model that we built above. Using a mini-batch size of $100$ and running it for $450$ epochs.  \n",
    "- For every epoch the training dataset is reshuffled, so that the examples in each mini-batches will be random from epoch to epoch.  \n",
    "- Cost for each mini-batch in an epoch is divided by the number of batches, so that the total cost at the end of the epoch will be an average of all the mini-batch costs. Cost is printed once every 20 epochs.\n",
    "- Training accuracy = $98.4\\%$\n",
    "- Test accuracy = $96.1\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Train Accuracy: 0.78752726\n",
      "Initial Test Accuracy: 0.7816\n",
      "============================================================================================\n",
      "Cost after epoch 19: 0.507029 , Train Accuracy: 0.858 , Test Accuracy: 0.86\n",
      "Cost after epoch 39: 0.323124 , Train Accuracy: 0.908 , Test Accuracy: 0.906\n",
      "Cost after epoch 59: 0.244721 , Train Accuracy: 0.931 , Test Accuracy: 0.925\n",
      "Cost after epoch 79: 0.204769 , Train Accuracy: 0.942 , Test Accuracy: 0.936\n",
      "Cost after epoch 99: 0.180146 , Train Accuracy: 0.947 , Test Accuracy: 0.94\n",
      "Cost after epoch 119: 0.162487 , Train Accuracy: 0.955 , Test Accuracy: 0.948\n",
      "Cost after epoch 139: 0.147616 , Train Accuracy: 0.955 , Test Accuracy: 0.945\n",
      "Cost after epoch 159: 0.137356 , Train Accuracy: 0.96 , Test Accuracy: 0.95\n",
      "Cost after epoch 179: 0.128360 , Train Accuracy: 0.962 , Test Accuracy: 0.952\n",
      "Cost after epoch 199: 0.119852 , Train Accuracy: 0.967 , Test Accuracy: 0.952\n",
      "Cost after epoch 219: 0.112679 , Train Accuracy: 0.963 , Test Accuracy: 0.95\n",
      "Cost after epoch 239: 0.106296 , Train Accuracy: 0.968 , Test Accuracy: 0.952\n",
      "Cost after epoch 259: 0.100406 , Train Accuracy: 0.97 , Test Accuracy: 0.953\n",
      "Cost after epoch 279: 0.094048 , Train Accuracy: 0.973 , Test Accuracy: 0.957\n",
      "Cost after epoch 299: 0.089826 , Train Accuracy: 0.976 , Test Accuracy: 0.956\n",
      "Cost after epoch 319: 0.084844 , Train Accuracy: 0.976 , Test Accuracy: 0.957\n",
      "Cost after epoch 339: 0.080149 , Train Accuracy: 0.976 , Test Accuracy: 0.958\n",
      "Cost after epoch 359: 0.075792 , Train Accuracy: 0.979 , Test Accuracy: 0.957\n",
      "Cost after epoch 379: 0.071695 , Train Accuracy: 0.981 , Test Accuracy: 0.958\n",
      "Cost after epoch 399: 0.068237 , Train Accuracy: 0.981 , Test Accuracy: 0.958\n",
      "Cost after epoch 419: 0.065140 , Train Accuracy: 0.979 , Test Accuracy: 0.957\n",
      "Cost after epoch 439: 0.061353 , Train Accuracy: 0.985 , Test Accuracy: 0.96\n",
      "Final Train Accuracy: 0.984\n",
      "Final Test Accuracy: 0.9609\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "np.random.seed(1) #setting seed to reproduce same results\n",
    "num_epochs = 450\n",
    "correct_preds1 = tf.equal(tf.argmax(zw6,1), tf.argmax(Y,1))\n",
    "accuracy1 = tf.reduce_mean(tf.cast(correct_preds1, tf.float32))\n",
    "print(\"Initial Train Accuracy:\", accuracy1.eval(session = sess, feed_dict={X: mnist.train.images, Y: mnist.train.labels}))\n",
    "print(\"Initial Test Accuracy:\", accuracy1.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))\n",
    "print(\"============================================================================================\")\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_cost = 0\n",
    "    perm = np.random.permutation(train_x.shape[0])\n",
    "    train_x = train_x[perm, :]\n",
    "    train_y = train_y[perm, :]\n",
    "    for i in range(num_batches):\n",
    "        #batch = mnist.train.next_batch(100)\n",
    "        start = i*batch_size\n",
    "        if i == num_batches-1:\n",
    "            batch_ip = train_x[start:train_x.shape[0], :]\n",
    "            batch_op = train_y[start:train_y.shape[0], :]\n",
    "        else:\n",
    "            batch_ip = train_x[start:start+batch_size, :]\n",
    "            batch_op = train_y[start:start+batch_size, :]\n",
    "        _, batch_cost = sess.run([train_step1, cost1], feed_dict={X:batch_ip, Y:batch_op})\n",
    "        epoch_cost += batch_cost/num_batches\n",
    "    if (epoch+1)%20 == 0:\n",
    "        acc_tr = accuracy1.eval(session = sess, feed_dict={X: mnist.train.images, Y: mnist.train.labels})\n",
    "        acc_test = accuracy1.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels})\n",
    "        print(\"Cost after epoch %i: %f\" % (epoch, epoch_cost), \", Train Accuracy:\", round(acc_tr,3), \", Test Accuracy:\", round(acc_test,3))\n",
    "print(\"Final Train Accuracy:\", accuracy1.eval(session = sess, feed_dict={X: mnist.train.images, Y: mnist.train.labels}))\n",
    "print(\"Final Test Accuracy:\", accuracy1.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "- Total number of network parameters is reduced from 5 million to 0.215 million, without significantly affecting test error.\n",
    "- Though it takes more time to train, it is a good trade-off to achieve significant reduction in network storage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
