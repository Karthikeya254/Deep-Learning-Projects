{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Denoising using RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Training Data\n",
    "Reading the processed training data from pickle files, that are saved earlier (data_prep_part2.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_spect_train = pickle.load(open('train_source', 'rb'))\n",
    "n_spect_train = pickle.load(open('train_noise', 'rb'))\n",
    "x_spect_train = pickle.load(open('train_mix', 'rb'))\n",
    "s_train_vec = pickle.load(open('istft_source', 'rb'))\n",
    "x_phase_train = pickle.load(open('train_mix_phase', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Validation Data\n",
    "Reading the processed validation data from pickle files, that are saved earlier (data_prep_part2.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_spect_valid = pickle.load(open('valid_source', 'rb'))\n",
    "n_spect_valid = pickle.load(open('valid_noise', 'rb'))\n",
    "x_spect_valid = pickle.load(open('valid_mix', 'rb'))\n",
    "s_valid_vec = pickle.load(open('istft_valid', 'rb'))\n",
    "x_phase_valid = pickle.load(open('valid_mix_phase', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM matrix\n",
    "- Number of timesteps are fixed to 150, so any signal that is less than this values is padded and any signal that is greater than this value is truncated.\n",
    "- IBM matrix is calculated on this modified data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_spect_train = [] #list of IBM matrices for signals in training data\n",
    "for i in range(len(s_spect_train)):\n",
    "    pad_len = time_steps - s_spect_train[i].shape[1]\n",
    "    if(pad_len <= 0):\n",
    "        s_spect_train[i] = s_spect_train[i][:,0:time_steps]\n",
    "        n_spect_train[i] = n_spect_train[i][:,0:time_steps]\n",
    "        x_spect_train[i] = x_spect_train[i][:,0:time_steps]\n",
    "        x_phase_train[i] = x_phase_train[i][:,0:time_steps]\n",
    "    else:\n",
    "        s_spect_train[i] = np.pad(s_spect_train[i], ((0,0),(0,pad_len)), 'constant')\n",
    "        n_spect_train[i] = np.pad(n_spect_train[i], ((0,0),(0,pad_len)), 'constant')\n",
    "        x_spect_train[i] = np.pad(x_spect_train[i], ((0,0),(0,pad_len)), 'constant')\n",
    "        x_phase_train[i] = np.pad(x_phase_train[i], ((0,0),(0,pad_len)), 'constant')\n",
    "    y_spect_train.append((s_spect_train[i] > n_spect_train[i]).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_spect_valid = [] #list of IBM matrices for signals in validation data\n",
    "for i in range(len(s_spect_valid)):\n",
    "    pad_len = time_steps - s_spect_valid[i].shape[1]\n",
    "    if(pad_len <= 0):\n",
    "        s_spect_valid[i] = s_spect_valid[i][:,0:time_steps]\n",
    "        n_spect_valid[i] = n_spect_valid[i][:,0:time_steps]\n",
    "        x_spect_valid[i] = x_spect_valid[i][:,0:time_steps]\n",
    "        x_phase_valid[i] = x_phase_valid[i][:,0:time_steps]\n",
    "    else:\n",
    "        s_spect_valid[i] = np.pad(s_spect_valid[i], ((0,0),(0,pad_len)), 'constant')\n",
    "        n_spect_valid[i] = np.pad(n_spect_valid[i], ((0,0),(0,pad_len)), 'constant')\n",
    "        x_spect_valid[i] = np.pad(x_spect_valid[i], ((0,0),(0,pad_len)), 'constant')\n",
    "        x_phase_valid[i] = np.pad(x_phase_valid[i], ((0,0),(0,pad_len)), 'constant')\n",
    "    y_spect_valid.append((s_spect_valid[i] > n_spect_valid[i]).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have a list of 2D numpy arrays, they are coverted to 3D arrays to make it compatible with the model. Now both input and output data will be in format \\[batch_size, time_steps, features\\] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_spect_train_arr = np.dstack(x_spect_train)\n",
    "y_spect_train_arr = np.dstack(y_spect_train)\n",
    "x_spect_train_arr=np.rollaxis(x_spect_train_arr,-1)\n",
    "x_spect_train_arr = np.transpose(x_spect_train_arr, (0,2,1))\n",
    "y_spect_train_arr=np.rollaxis(y_spect_train_arr,-1)\n",
    "y_spect_train_arr = np.transpose(y_spect_train_arr, (0,2,1))\n",
    "x_phase_train_arr = np.dstack(x_phase_train)\n",
    "x_phase_train_arr = np.rollaxis(x_phase_train_arr,-1)\n",
    "x_phase_train_arr = np.transpose(x_phase_train_arr, (0,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_spect_valid_arr = np.dstack(x_spect_valid)\n",
    "y_spect_valid_arr = np.dstack(y_spect_valid)\n",
    "x_spect_valid_arr=np.rollaxis(x_spect_valid_arr,-1)\n",
    "x_spect_valid_arr = np.transpose(x_spect_valid_arr, (0,2,1))\n",
    "y_spect_valid_arr=np.rollaxis(y_spect_valid_arr,-1)\n",
    "y_spect_valid_arr = np.transpose(y_spect_valid_arr, (0,2,1))\n",
    "x_phase_valid_arr = np.dstack(x_phase_valid)\n",
    "x_phase_valid_arr = np.rollaxis(x_phase_valid_arr,-1)\n",
    "x_phase_valid_arr = np.transpose(x_phase_valid_arr, (0,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 150, 513)\n",
      "(1200, 150, 513)\n",
      "(1200, 150, 513)\n"
     ]
    }
   ],
   "source": [
    "print(x_spect_train_arr.shape)\n",
    "print(y_spect_train_arr.shape)\n",
    "print(x_phase_train_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 150, 513)\n",
      "(1200, 150, 513)\n",
      "(1200, 150, 513)\n"
     ]
    }
   ],
   "source": [
    "print(x_spect_valid_arr.shape)\n",
    "print(y_spect_valid_arr.shape)\n",
    "print(x_phase_valid_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "- Placeholders: Both input and output data has similar dimensions as we are doing a sort of regression problem. As the batch_size could vary it is given as None in the placeholder.\n",
    "- During execution a batch_size of 10 is used.\n",
    "- 2 RNN layers are stacked in the network. The first cell hass 500 units and the second cell has 513 units so as to match out output feature dimension and avoid a fully connected layer.\n",
    "- GRU cells are used in both rnn layers. The activation the the second layer is changed to sigmoid as out output is binary valued.\n",
    "- Mean squared error is used in the cost function, as it is a sort of regression problem.\n",
    "- Adagrad optimizer is used with a learning rate of $0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "input_dim = 513"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size x time steps x features.\n",
    "data = tf.placeholder(tf.float32, [None, time_steps, input_dim])\n",
    "y = tf.placeholder(tf.float32, [None, time_steps, input_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_sizes = [500,513]\n",
    "GRUs = [tf.contrib.rnn.GRUCell(size, activation=tf.nn.sigmoid) if size==513 else tf.contrib.rnn.GRUCell(size) for size in gru_sizes]\n",
    "#drops = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=0.8) for lstm in lstms]\n",
    "cell = tf.contrib.rnn.MultiRNNCell(GRUs)\n",
    "# initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "output, state = tf.nn.dynamic_rnn(cell, data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.losses.mean_squared_error(output, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(0.5).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting configurtion for tensorflow memory usage\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.33\n",
    "\n",
    "#Creating a tendorflow session\n",
    "sess=tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing Model\n",
    "Model is executed for 1000 epochs, and SNR values for validation signals are computed for every 100 epochs.\n",
    "\n",
    "As I have used truncated signals, the predicted signal will have different size than the ground truth source signals depending on the length of original signal. So I have computed two different SNR values for the validation data, first one (\"SNR validation\") keeping the source signal as it is and padding or truncating the predicted signal to match the size of the source signal. In the second one (\"SNR validation trunc\"), I took the minimum size between both source and predicted signals and truncated the longer one.\n",
    "\n",
    "The first approach is more reliable and it produces a SNR value of $10.5$ for the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs Done:  9\n",
      "Epochs Done:  19\n",
      "Epochs Done:  29\n",
      "Epochs Done:  39\n",
      "Epochs Done:  49\n",
      "Epochs Done:  59\n",
      "Epochs Done:  69\n",
      "Epochs Done:  79\n",
      "Epochs Done:  89\n",
      "Epochs Done:  99\n",
      "Epoch: 99 , SNR validation: 8.923 , SNR validation trunc: 9.411\n",
      "====================================================\n",
      "Epochs Done:  109\n",
      "Epochs Done:  119\n",
      "Epochs Done:  129\n",
      "Epochs Done:  139\n",
      "Epochs Done:  149\n",
      "Epochs Done:  159\n",
      "Epochs Done:  169\n",
      "Epochs Done:  179\n",
      "Epochs Done:  189\n",
      "Epochs Done:  199\n",
      "Epoch: 199 , SNR validation: 9.761 , SNR validation trunc: 10.314\n",
      "====================================================\n",
      "Epochs Done:  209\n",
      "Epochs Done:  219\n",
      "Epochs Done:  229\n",
      "Epochs Done:  239\n",
      "Epochs Done:  249\n",
      "Epochs Done:  259\n",
      "Epochs Done:  269\n",
      "Epochs Done:  279\n",
      "Epochs Done:  289\n",
      "Epochs Done:  299\n",
      "Epoch: 299 , SNR validation: 10.126 , SNR validation trunc: 10.709\n",
      "====================================================\n",
      "Epochs Done:  309\n",
      "Epochs Done:  319\n",
      "Epochs Done:  329\n",
      "Epochs Done:  339\n",
      "Epochs Done:  349\n",
      "Epochs Done:  359\n",
      "Epochs Done:  369\n",
      "Epochs Done:  379\n",
      "Epochs Done:  389\n",
      "Epochs Done:  399\n",
      "Epoch: 399 , SNR validation: 10.275 , SNR validation trunc: 10.871\n",
      "====================================================\n",
      "Epochs Done:  409\n",
      "Epochs Done:  419\n",
      "Epochs Done:  429\n",
      "Epochs Done:  439\n",
      "Epochs Done:  449\n",
      "Epochs Done:  459\n",
      "Epochs Done:  469\n",
      "Epochs Done:  479\n",
      "Epochs Done:  489\n",
      "Epochs Done:  499\n",
      "Epoch: 499 , SNR validation: 10.357 , SNR validation trunc: 10.964\n",
      "====================================================\n",
      "Epochs Done:  509\n",
      "Epochs Done:  519\n",
      "Epochs Done:  529\n",
      "Epochs Done:  539\n",
      "Epochs Done:  549\n",
      "Epochs Done:  559\n",
      "Epochs Done:  569\n",
      "Epochs Done:  579\n",
      "Epochs Done:  589\n",
      "Epochs Done:  599\n",
      "Epoch: 599 , SNR validation: 10.386 , SNR validation trunc: 10.995\n",
      "====================================================\n",
      "Epochs Done:  609\n",
      "Epochs Done:  619\n",
      "Epochs Done:  629\n",
      "Epochs Done:  639\n",
      "Epochs Done:  649\n",
      "Epochs Done:  659\n",
      "Epochs Done:  669\n",
      "Epochs Done:  679\n",
      "Epochs Done:  689\n",
      "Epochs Done:  699\n",
      "Epoch: 699 , SNR validation: 10.425 , SNR validation trunc: 11.038\n",
      "====================================================\n",
      "Epochs Done:  709\n",
      "Epochs Done:  719\n",
      "Epochs Done:  729\n",
      "Epochs Done:  739\n",
      "Epochs Done:  749\n",
      "Epochs Done:  759\n",
      "Epochs Done:  769\n",
      "Epochs Done:  779\n",
      "Epochs Done:  789\n",
      "Epochs Done:  799\n",
      "Epoch: 799 , SNR validation: 10.469 , SNR validation trunc: 11.084\n",
      "====================================================\n",
      "Epochs Done:  809\n",
      "Epochs Done:  819\n",
      "Epochs Done:  829\n",
      "Epochs Done:  839\n",
      "Epochs Done:  849\n",
      "Epochs Done:  859\n",
      "Epochs Done:  869\n",
      "Epochs Done:  879\n",
      "Epochs Done:  889\n",
      "Epochs Done:  899\n",
      "Epoch: 899 , SNR validation: 10.498 , SNR validation trunc: 11.114\n",
      "====================================================\n",
      "Epochs Done:  909\n",
      "Epochs Done:  919\n",
      "Epochs Done:  929\n",
      "Epochs Done:  939\n",
      "Epochs Done:  949\n",
      "Epochs Done:  959\n",
      "Epochs Done:  969\n",
      "Epochs Done:  979\n",
      "Epochs Done:  989\n",
      "Epochs Done:  999\n",
      "Epoch: 999 , SNR validation: 10.515 , SNR validation trunc: 11.133\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "num_epochs = 1000\n",
    "num_batches = int(x_spect_train_arr.shape[0]/batch_size)\n",
    "strt = time.clock()\n",
    "for epoch in range(num_epochs):\n",
    "    estrt = time.clock()\n",
    "    epoch_cost = 0\n",
    "    for i in range(num_batches):\n",
    "        start = i*batch_size\n",
    "        batch_ip = x_spect_train_arr[start:start+batch_size, :, :]\n",
    "        batch_op = y_spect_train_arr[start:start+batch_size, :, :]\n",
    "        sess.run(optimizer, feed_dict={data:batch_ip, y:batch_op})\n",
    "    snr_p_v = 0\n",
    "    snr_p_v_min = 0\n",
    "    if (epoch+1)%10 == 0: print(\"Epochs Done: \", epoch)\n",
    "    if (epoch+1)%100 == 0:\n",
    "        y_p_v = sess.run(output, feed_dict={data:x_spect_valid_arr[:,:,:], y:y_spect_valid_arr[:,:,:]})\n",
    "        xy_p_v = y_p_v * x_spect_valid_arr[:,:,:] * x_phase_valid_arr[:,:,:]\n",
    "        for j in range(1200):\n",
    "            xy_test_p_v = librosa.istft(xy_p_v[j,:,:].T, hop_length=512)\n",
    "            numer_v = np.sum(np.square(s_valid_vec[j]))\n",
    "            pad_p_v = len(s_valid_vec[j]) - len(xy_test_p_v)\n",
    "            min_p_v = min(len(s_valid_vec[j]), len(xy_test_p_v))\n",
    "            denom_p_v_min = np.sum(np.square(s_valid_vec[j][:min_p_v] - xy_test_p_v[:min_p_v]))\n",
    "            if pad_p_v > 0: \n",
    "                xy_test_p_v = np.pad(xy_test_p_v, (0,pad_p_v), 'constant')\n",
    "                denom_p_v = np.sum(np.square(s_valid_vec[j] - xy_test_p_v))\n",
    "            else:\n",
    "                denom_p_v = np.sum(np.square(s_valid_vec[j] - xy_test_p_v[0:len(s_valid_vec[j])]))\n",
    "            snr_p_v += 10*np.log10(numer_v/denom_p_v)\n",
    "            snr_p_v_min += 10*np.log10(numer_v/denom_p_v_min)\n",
    "        print(\"Epoch:\", epoch, \", SNR validation:\", round(snr_p_v/1200,3), \", SNR validation trunc:\", round(snr_p_v_min/1200,3))\n",
    "        print(\"====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Test data is read from the earlier stored pickle files. The data is passed to above model to compute the prediction mask. It is then multiplied to the spectrogram and its phase information, later inverse STFT is applied to this and saved to audio files.\n",
    "\n",
    "The file naming is in format **denoise\\_tex$<$filenumber$>$.wav** files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_spect_test = pickle.load(open('test_mix', 'rb'))\n",
    "x_phase_test = pickle.load(open('test_mix_phase', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_spect_test)):\n",
    "    pad_len = time_steps - x_spect_test[i].shape[1]\n",
    "    if(pad_len <= 0):\n",
    "        x_spect_test[i] = x_spect_test[i][:,0:time_steps]\n",
    "        x_phase_test[i] = x_phase_test[i][:,0:time_steps]\n",
    "    else:\n",
    "        x_spect_test[i] = np.pad(x_spect_test[i], ((0,0),(0,pad_len)), 'constant')\n",
    "        x_phase_test[i] = np.pad(x_phase_test[i], ((0,0),(0,pad_len)), 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_spect_test_arr = np.dstack(x_spect_test)\n",
    "x_spect_test_arr = np.rollaxis(x_spect_test_arr,-1)\n",
    "x_spect_test_arr = np.transpose(x_spect_test_arr, (0,2,1))\n",
    "x_phase_test_arr = np.dstack(x_phase_test)\n",
    "x_phase_test_arr = np.rollaxis(x_phase_test_arr,-1)\n",
    "x_phase_test_arr = np.transpose(x_phase_test_arr, (0,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 150, 513)\n",
      "(400, 150, 513)\n"
     ]
    }
   ],
   "source": [
    "print(x_spect_test_arr.shape)\n",
    "print(x_phase_test_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = '/opt/e533/timit-homework/te/'\n",
    "x_files_test = []\n",
    "x_files_test_denoise = []\n",
    "for f in listdir(test_path):\n",
    "    if isfile(join(test_path, f)):\n",
    "        x_files_test.append(join(test_path,f))\n",
    "        x_files_test_denoise.append(join('dtest','denoise_'+f))\n",
    "x_files_test.sort()\n",
    "x_files_test_denoise.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p_te = sess.run(output, feed_dict={data:x_spect_test_arr[:,:,:]})\n",
    "xy_p_te = y_p_te * x_spect_test_arr[:,:,:] * x_phase_test_arr[:,:,:]\n",
    "for j in range(400):\n",
    "    sn, sr=librosa.load(x_files_test[j])\n",
    "    xy_test_p_te = librosa.istft(xy_p_te[j,:,:].T, hop_length=512)\n",
    "    librosa.output.write_wav(x_files_test_denoise[j], xy_test_p_te, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://www.tensorflow.org/tutorials/recurrent\n",
    "2. https://danijar.com/introduction-to-recurrent-networks-in-tensorflow/\n",
    "3. https://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
